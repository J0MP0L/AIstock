{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('NNN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;DATE&gt;</th>\n",
       "      <th>&lt;TIME&gt;</th>\n",
       "      <th>n_o</th>\n",
       "      <th>n_h</th>\n",
       "      <th>n_l</th>\n",
       "      <th>n_c</th>\n",
       "      <th>&lt;TICKVOL&gt;</th>\n",
       "      <th>n_v</th>\n",
       "      <th>&lt;SPREAD&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>116.16</td>\n",
       "      <td>116.19</td>\n",
       "      <td>116.16</td>\n",
       "      <td>116.19</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:01:00</td>\n",
       "      <td>116.19</td>\n",
       "      <td>116.19</td>\n",
       "      <td>116.05</td>\n",
       "      <td>116.08</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:02:00</td>\n",
       "      <td>116.10</td>\n",
       "      <td>116.20</td>\n",
       "      <td>116.08</td>\n",
       "      <td>116.16</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:03:00</td>\n",
       "      <td>116.16</td>\n",
       "      <td>116.28</td>\n",
       "      <td>116.16</td>\n",
       "      <td>116.22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:04:00</td>\n",
       "      <td>116.23</td>\n",
       "      <td>116.39</td>\n",
       "      <td>116.22</td>\n",
       "      <td>116.39</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45453</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:40:00</td>\n",
       "      <td>133.12</td>\n",
       "      <td>133.33</td>\n",
       "      <td>133.12</td>\n",
       "      <td>133.18</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45454</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:41:00</td>\n",
       "      <td>133.19</td>\n",
       "      <td>133.26</td>\n",
       "      <td>133.09</td>\n",
       "      <td>133.14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45455</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:42:00</td>\n",
       "      <td>133.14</td>\n",
       "      <td>133.18</td>\n",
       "      <td>132.84</td>\n",
       "      <td>132.84</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45456</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:43:00</td>\n",
       "      <td>132.85</td>\n",
       "      <td>133.04</td>\n",
       "      <td>132.84</td>\n",
       "      <td>132.98</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45457</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:44:00</td>\n",
       "      <td>132.97</td>\n",
       "      <td>133.06</td>\n",
       "      <td>132.91</td>\n",
       "      <td>133.05</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45458 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           <DATE>    <TIME>     n_o     n_h     n_l     n_c  <TICKVOL>  n_v  \\\n",
       "0      2024.09.03  10:00:00  116.16  116.19  116.16  116.19          7    0   \n",
       "1      2024.09.03  10:01:00  116.19  116.19  116.05  116.08         11    0   \n",
       "2      2024.09.03  10:02:00  116.10  116.20  116.08  116.16         14    0   \n",
       "3      2024.09.03  10:03:00  116.16  116.28  116.16  116.22         13    0   \n",
       "4      2024.09.03  10:04:00  116.23  116.39  116.22  116.39         17    0   \n",
       "...           ...       ...     ...     ...     ...     ...        ...  ...   \n",
       "45453  2024.12.20  20:40:00  133.12  133.33  133.12  133.18         61    0   \n",
       "45454  2024.12.20  20:41:00  133.19  133.26  133.09  133.14         67    0   \n",
       "45455  2024.12.20  20:42:00  133.14  133.18  132.84  132.84         60    0   \n",
       "45456  2024.12.20  20:43:00  132.85  133.04  132.84  132.98         79    0   \n",
       "45457  2024.12.20  20:44:00  132.97  133.06  132.91  133.05         63    0   \n",
       "\n",
       "       <SPREAD>  \n",
       "0            91  \n",
       "1            91  \n",
       "2            91  \n",
       "3            91  \n",
       "4            91  \n",
       "...         ...  \n",
       "45453        16  \n",
       "45454        16  \n",
       "45455        16  \n",
       "45456        16  \n",
       "45457        16  \n",
       "\n",
       "[45458 rows x 9 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diff_n_o\"]=[None]+[np.log(df[\"n_o\"][i+1])-np.log(df[\"n_o\"][i]) for i in range(len(df)-1)]\n",
    "df[\"diff_n_h\"]=[None]+[np.log(df[\"n_h\"][i+1])-np.log(df[\"n_h\"][i]) for i in range(len(df)-1)]\n",
    "df[\"diff_n_c\"]=[None]+[np.log(df[\"n_c\"][i+1])-np.log(df[\"n_c\"][i]) for i in range(len(df)-1)]\n",
    "\n",
    "df[\"diff_n_l\"]=[None]+[np.log(df[\"n_l\"][i+1])-np.log(df[\"n_l\"][i]) for i in range(len(df)-1)]\n",
    "df[\"diff_n_OH\"]=[None]+[np.log(df[\"n_h\"][i+1])-np.log(df[\"n_o\"][i+1]) for i in range(len(df)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;DATE&gt;</th>\n",
       "      <th>&lt;TIME&gt;</th>\n",
       "      <th>n_o</th>\n",
       "      <th>n_h</th>\n",
       "      <th>n_l</th>\n",
       "      <th>n_c</th>\n",
       "      <th>&lt;TICKVOL&gt;</th>\n",
       "      <th>n_v</th>\n",
       "      <th>&lt;SPREAD&gt;</th>\n",
       "      <th>diff_n_o</th>\n",
       "      <th>diff_n_h</th>\n",
       "      <th>diff_n_c</th>\n",
       "      <th>diff_n_l</th>\n",
       "      <th>diff_n_OH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:01:00</td>\n",
       "      <td>116.19</td>\n",
       "      <td>116.19</td>\n",
       "      <td>116.05</td>\n",
       "      <td>116.08</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:02:00</td>\n",
       "      <td>116.10</td>\n",
       "      <td>116.20</td>\n",
       "      <td>116.08</td>\n",
       "      <td>116.16</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>-0.000775</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:03:00</td>\n",
       "      <td>116.16</td>\n",
       "      <td>116.28</td>\n",
       "      <td>116.16</td>\n",
       "      <td>116.22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:04:00</td>\n",
       "      <td>116.23</td>\n",
       "      <td>116.39</td>\n",
       "      <td>116.22</td>\n",
       "      <td>116.39</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.001376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024.09.03</td>\n",
       "      <td>10:05:00</td>\n",
       "      <td>116.38</td>\n",
       "      <td>116.44</td>\n",
       "      <td>116.37</td>\n",
       "      <td>116.38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45453</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:40:00</td>\n",
       "      <td>133.12</td>\n",
       "      <td>133.33</td>\n",
       "      <td>133.12</td>\n",
       "      <td>133.18</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.001726</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45454</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:41:00</td>\n",
       "      <td>133.19</td>\n",
       "      <td>133.26</td>\n",
       "      <td>133.09</td>\n",
       "      <td>133.14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45455</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:42:00</td>\n",
       "      <td>133.14</td>\n",
       "      <td>133.18</td>\n",
       "      <td>132.84</td>\n",
       "      <td>132.84</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.000375</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.002256</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45456</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:43:00</td>\n",
       "      <td>132.85</td>\n",
       "      <td>133.04</td>\n",
       "      <td>132.84</td>\n",
       "      <td>132.98</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.002181</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45457</th>\n",
       "      <td>2024.12.20</td>\n",
       "      <td>20:44:00</td>\n",
       "      <td>132.97</td>\n",
       "      <td>133.06</td>\n",
       "      <td>132.91</td>\n",
       "      <td>133.05</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45457 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           <DATE>    <TIME>     n_o     n_h     n_l     n_c  <TICKVOL>  n_v  \\\n",
       "1      2024.09.03  10:01:00  116.19  116.19  116.05  116.08         11    0   \n",
       "2      2024.09.03  10:02:00  116.10  116.20  116.08  116.16         14    0   \n",
       "3      2024.09.03  10:03:00  116.16  116.28  116.16  116.22         13    0   \n",
       "4      2024.09.03  10:04:00  116.23  116.39  116.22  116.39         17    0   \n",
       "5      2024.09.03  10:05:00  116.38  116.44  116.37  116.38         13    0   \n",
       "...           ...       ...     ...     ...     ...     ...        ...  ...   \n",
       "45453  2024.12.20  20:40:00  133.12  133.33  133.12  133.18         61    0   \n",
       "45454  2024.12.20  20:41:00  133.19  133.26  133.09  133.14         67    0   \n",
       "45455  2024.12.20  20:42:00  133.14  133.18  132.84  132.84         60    0   \n",
       "45456  2024.12.20  20:43:00  132.85  133.04  132.84  132.98         79    0   \n",
       "45457  2024.12.20  20:44:00  132.97  133.06  132.91  133.05         63    0   \n",
       "\n",
       "       <SPREAD>  diff_n_o  diff_n_h  diff_n_c  diff_n_l  diff_n_OH  \n",
       "1            91  0.000258  0.000000 -0.000947 -0.000947   0.000000  \n",
       "2            91 -0.000775  0.000086  0.000689  0.000258   0.000861  \n",
       "3            91  0.000517  0.000688  0.000516  0.000689   0.001033  \n",
       "4            91  0.000602  0.000946  0.001462  0.000516   0.001376  \n",
       "5            91  0.001290  0.000429 -0.000086  0.001290   0.000515  \n",
       "...         ...       ...       ...       ...       ...        ...  \n",
       "45453        16 -0.001726 -0.000450  0.000300  0.000376   0.001576  \n",
       "45454        16  0.000526 -0.000525 -0.000300 -0.000225   0.000525  \n",
       "45455        16 -0.000375 -0.000601 -0.002256 -0.001880   0.000300  \n",
       "45456        16 -0.002181 -0.001052  0.001053  0.000000   0.001429  \n",
       "45457        16  0.000903  0.000150  0.000526  0.000527   0.000677  \n",
       "\n",
       "[45457 rows x 14 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_n_o</th>\n",
       "      <th>diff_n_l</th>\n",
       "      <th>diff_n_h</th>\n",
       "      <th>diff_n_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000775</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>-0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45452</th>\n",
       "      <td>0.000975</td>\n",
       "      <td>-0.001127</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.001426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45453</th>\n",
       "      <td>-0.001726</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45454</th>\n",
       "      <td>0.000526</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45455</th>\n",
       "      <td>-0.000375</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45456</th>\n",
       "      <td>-0.002181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45455 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diff_n_o  diff_n_l  diff_n_h  diff_n_c\n",
       "2     -0.000775  0.000258  0.000086  0.000689\n",
       "3      0.000517  0.000689  0.000688  0.000516\n",
       "4      0.000602  0.000516  0.000946  0.001462\n",
       "5      0.001290  0.001290  0.000429 -0.000086\n",
       "6      0.000000 -0.000946 -0.000429 -0.001032\n",
       "...         ...       ...       ...       ...\n",
       "45452  0.000975 -0.001127  0.000375 -0.001426\n",
       "45453 -0.001726  0.000376 -0.000450  0.000300\n",
       "45454  0.000526 -0.000225 -0.000525 -0.000300\n",
       "45455 -0.000375 -0.001880 -0.000601 -0.002256\n",
       "45456 -0.002181  0.000000 -0.001052  0.001053\n",
       "\n",
       "[45455 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xl1=data[[\"diff_n_o\",\"diff_n_l\",\"diff_n_h\",\"diff_n_c\"]].iloc[1:-1,:]\n",
    "Xl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_n_o</th>\n",
       "      <th>diff_n_l</th>\n",
       "      <th>diff_n_h</th>\n",
       "      <th>diff_n_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000775</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45451</th>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45452</th>\n",
       "      <td>0.000975</td>\n",
       "      <td>-0.001127</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.001426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45453</th>\n",
       "      <td>-0.001726</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45454</th>\n",
       "      <td>0.000526</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45455</th>\n",
       "      <td>-0.000375</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.002256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45455 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diff_n_o  diff_n_l  diff_n_h  diff_n_c\n",
       "1      0.000258 -0.000947  0.000000 -0.000947\n",
       "2     -0.000775  0.000258  0.000086  0.000689\n",
       "3      0.000517  0.000689  0.000688  0.000516\n",
       "4      0.000602  0.000516  0.000946  0.001462\n",
       "5      0.001290  0.001290  0.000429 -0.000086\n",
       "...         ...       ...       ...       ...\n",
       "45451  0.000150  0.000375  0.000225  0.000900\n",
       "45452  0.000975 -0.001127  0.000375 -0.001426\n",
       "45453 -0.001726  0.000376 -0.000450  0.000300\n",
       "45454  0.000526 -0.000225 -0.000525 -0.000300\n",
       "45455 -0.000375 -0.001880 -0.000601 -0.002256\n",
       "\n",
       "[45455 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xl2=data[[\"diff_n_o\",\"diff_n_l\",\"diff_n_h\",\"diff_n_c\"]].iloc[0:-2,:]\n",
    "Xl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.concatenate([Xl1,Xl2],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep=10\n",
    "X=[]\n",
    "for i in range(len(x)-timestep+1):\n",
    "    c=np.array(x[i:i+timestep])\n",
    "    X.append(c)\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45446, 10, 8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X[:40000-1]\n",
    "X_test=X[40000-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data[\"diff_n_h\"].iloc[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11      -0.000172\n",
       "12       0.000000\n",
       "13      -0.000430\n",
       "14       0.000516\n",
       "15       0.001375\n",
       "           ...   \n",
       "45453   -0.000450\n",
       "45454   -0.000525\n",
       "45455   -0.000601\n",
       "45456   -0.001052\n",
       "45457    0.000150\n",
       "Name: diff_n_h, Length: 45447, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=Y[1:40000]\n",
    "Y_test=Y[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39999 39999\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train),len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45453   -0.000450\n",
       " 45454   -0.000525\n",
       " 45455   -0.000601\n",
       " 45456   -0.001052\n",
       " 45457    0.000150\n",
       " Name: diff_n_h, dtype: float64,\n",
       " array([[-1.42723029e-03, -3.00819736e-04, -1.35125011e-03,\n",
       "         -4.51059999e-04, -1.65004163e-03, -1.65289294e-03,\n",
       "         -1.04971143e-03, -1.27689956e-03],\n",
       "        [-4.51127827e-04,  1.50421180e-04,  6.00781033e-04,\n",
       "          9.77039648e-04, -1.42723029e-03, -3.00819736e-04,\n",
       "         -1.35125011e-03, -4.51059999e-04],\n",
       "        [ 1.05231519e-03,  9.02052230e-04,  7.50469078e-04,\n",
       "          6.75853290e-04, -4.51127827e-04,  1.50421180e-04,\n",
       "          6.00781033e-04,  9.77039648e-04],\n",
       "        [ 6.75904047e-04,  6.00916416e-04,  7.50159409e-05,\n",
       "          0.00000000e+00,  1.05231519e-03,  9.02052230e-04,\n",
       "          7.50469078e-04,  6.75853290e-04],\n",
       "        [ 1.50138879e-04,  3.75389471e-04,  2.25014064e-04,\n",
       "          9.00427764e-04,  6.75904047e-04,  6.00916416e-04,\n",
       "          7.50159409e-05,  0.00000000e+00],\n",
       "        [ 9.75353643e-04, -1.12659143e-03,  3.74910963e-04,\n",
       "         -1.42605195e-03,  1.50138879e-04,  3.75389471e-04,\n",
       "          2.25014064e-04,  9.00427764e-04],\n",
       "        [-1.72627356e-03,  3.75671517e-04, -4.49910026e-04,\n",
       "          3.00390510e-04,  9.75353643e-04, -1.12659143e-03,\n",
       "          3.74910963e-04, -1.42605195e-03],\n",
       "        [ 5.25703140e-04, -2.25385974e-04, -5.25150993e-04,\n",
       "         -3.00390510e-04, -1.72627356e-03,  3.75671517e-04,\n",
       "         -4.49910026e-04,  3.00390510e-04],\n",
       "        [-3.75474040e-04, -1.88019459e-03, -6.00510452e-04,\n",
       "         -2.25580966e-03,  5.25703140e-04, -2.25385974e-04,\n",
       "         -5.25150993e-04, -3.00390510e-04],\n",
       "        [-2.18053397e-03,  0.00000000e+00, -1.05176180e-03,\n",
       "          1.05334447e-03, -3.75474040e-04, -1.88019459e-03,\n",
       "         -6.00510452e-04, -2.25580966e-03]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[-5:],X_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"custom_activation\")   \n",
    "def custom_activation(inputs):\n",
    "        x_min = tf.minimum(inputs[:, :tf.shape(inputs)[1] // 2], 0)\n",
    "        x_max = tf.maximum(inputs[:, tf.shape(inputs)[1] // 2:], 0)\n",
    "        return tf.concat([x_min,x_max],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(512, return_sequences=True, activation=custom_activation),\n",
    "    tf.keras.layers.SimpleRNN(256, activation=custom_activation),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(128,activation=custom_activation),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\", name=\"acc\")\n",
    "def acc(y_true, y_pred):\n",
    "    YT = tf.cast(y_true > 0, tf.int8)\n",
    "    YP = tf.cast(y_pred > 0, tf.int8)\n",
    "    accuracy = tf.reduce_mean(tf.cast(YT == YP, tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.96\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True  # If True, decay in discrete steps\n",
    ")\n",
    "\n",
    "# Define the optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer ,loss=\"huber\",metrics=[\"mae\",acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'AIpred0.01.keras',             # ชื่อไฟล์ที่จะบันทึกโมเดล\n",
    "    monitor='val_loss',             # ติดตามการเปลี่ยนแปลงของ 'val_loss'\n",
    "    save_best_only=True,            # บันทึกเฉพาะโมเดลที่ดีที่สุด\n",
    "    save_weights_only=False,        # บันทึกทั้งโมเดล (ไม่ใช่แค่ weights)\n",
    "    mode='min',                     # หาก 'monitor' เป็น 'val_loss' เราต้องการค่า 'min' (ค่าต่ำสุด)\n",
    "    verbose=1                       # แสดงผลการบันทึกโมเดล\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6147 - loss: 3.6327e-07 - mae: 4.9685e-04\n",
      "Epoch 1: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step - acc: 0.6151 - loss: 3.6347e-07 - mae: 4.9651e-04 - val_acc: 0.6878 - val_loss: 5.4626e-07 - val_mae: 5.0972e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6362 - loss: 3.5173e-07 - mae: 4.7268e-04\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6362 - loss: 3.5212e-07 - mae: 4.7277e-04 - val_acc: 0.6751 - val_loss: 5.4745e-07 - val_mae: 5.1787e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - acc: 0.6354 - loss: 3.8495e-07 - mae: 4.7598e-04\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - acc: 0.6355 - loss: 3.8444e-07 - mae: 4.7592e-04 - val_acc: 0.6601 - val_loss: 5.4848e-07 - val_mae: 5.2707e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6408 - loss: 3.2499e-07 - mae: 4.6780e-04\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6406 - loss: 3.2607e-07 - mae: 4.6809e-04 - val_acc: 0.6806 - val_loss: 5.4475e-07 - val_mae: 5.1224e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6356 - loss: 3.6473e-07 - mae: 4.7422e-04\n",
      "Epoch 5: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6357 - loss: 3.6470e-07 - mae: 4.7419e-04 - val_acc: 0.6720 - val_loss: 5.4576e-07 - val_mae: 5.1882e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6406 - loss: 3.4219e-07 - mae: 4.7271e-04\n",
      "Epoch 6: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6405 - loss: 3.4271e-07 - mae: 4.7273e-04 - val_acc: 0.6850 - val_loss: 5.4243e-07 - val_mae: 5.0983e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6345 - loss: 3.7383e-07 - mae: 4.7800e-04\n",
      "Epoch 7: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6346 - loss: 3.7360e-07 - mae: 4.7795e-04 - val_acc: 0.6875 - val_loss: 5.4332e-07 - val_mae: 5.0522e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - acc: 0.6401 - loss: 3.4890e-07 - mae: 4.6948e-04\n",
      "Epoch 8: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.6401 - loss: 3.4926e-07 - mae: 4.6958e-04 - val_acc: 0.6690 - val_loss: 5.4741e-07 - val_mae: 5.2052e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6354 - loss: 3.3438e-07 - mae: 4.7044e-04\n",
      "Epoch 9: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6354 - loss: 3.3511e-07 - mae: 4.7053e-04 - val_acc: 0.6046 - val_loss: 6.1387e-07 - val_mae: 5.9380e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6107 - loss: 3.8053e-07 - mae: 5.0286e-04\n",
      "Epoch 10: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6111 - loss: 3.8022e-07 - mae: 5.0233e-04 - val_acc: 0.6856 - val_loss: 5.4134e-07 - val_mae: 5.0389e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6416 - loss: 3.3465e-07 - mae: 4.6807e-04\n",
      "Epoch 11: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.6416 - loss: 3.3530e-07 - mae: 4.6814e-04 - val_acc: 0.6844 - val_loss: 5.4043e-07 - val_mae: 5.0588e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - acc: 0.6312 - loss: 3.5278e-07 - mae: 4.8007e-04\n",
      "Epoch 12: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.6312 - loss: 3.5311e-07 - mae: 4.8007e-04 - val_acc: 0.6199 - val_loss: 5.5878e-07 - val_mae: 5.6173e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6349 - loss: 3.7039e-07 - mae: 4.7509e-04\n",
      "Epoch 13: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6350 - loss: 3.7019e-07 - mae: 4.7503e-04 - val_acc: 0.6374 - val_loss: 5.5130e-07 - val_mae: 5.4551e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6253 - loss: 3.6748e-07 - mae: 4.8803e-04\n",
      "Epoch 14: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6255 - loss: 3.6741e-07 - mae: 4.8777e-04 - val_acc: 0.6842 - val_loss: 5.4138e-07 - val_mae: 5.0193e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6406 - loss: 3.5850e-07 - mae: 4.6825e-04\n",
      "Epoch 15: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6407 - loss: 3.5855e-07 - mae: 4.6831e-04 - val_acc: 0.6675 - val_loss: 5.5004e-07 - val_mae: 5.0931e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - acc: 0.6378 - loss: 3.6759e-07 - mae: 4.7494e-04\n",
      "Epoch 16: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.6379 - loss: 3.6739e-07 - mae: 4.7486e-04 - val_acc: 0.6808 - val_loss: 5.4240e-07 - val_mae: 5.0173e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - acc: 0.6397 - loss: 3.5541e-07 - mae: 4.7167e-04\n",
      "Epoch 17: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - acc: 0.6398 - loss: 3.5551e-07 - mae: 4.7164e-04 - val_acc: 0.6642 - val_loss: 5.3975e-07 - val_mae: 5.1836e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6437 - loss: 3.8728e-07 - mae: 4.7240e-04\n",
      "Epoch 18: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6437 - loss: 3.8653e-07 - mae: 4.7227e-04 - val_acc: 0.6183 - val_loss: 5.5512e-07 - val_mae: 5.5849e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - acc: 0.6228 - loss: 3.9150e-07 - mae: 4.9056e-04\n",
      "Epoch 19: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.6231 - loss: 3.9077e-07 - mae: 4.9022e-04 - val_acc: 0.6876 - val_loss: 5.3511e-07 - val_mae: 5.0049e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6449 - loss: 3.5872e-07 - mae: 4.7211e-04\n",
      "Epoch 20: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6448 - loss: 3.5870e-07 - mae: 4.7206e-04 - val_acc: 0.6789 - val_loss: 5.4313e-07 - val_mae: 5.0200e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - acc: 0.6417 - loss: 3.9862e-07 - mae: 4.7085e-04\n",
      "Epoch 21: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.6417 - loss: 3.9757e-07 - mae: 4.7077e-04 - val_acc: 0.6864 - val_loss: 5.3432e-07 - val_mae: 5.0038e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6365 - loss: 3.5149e-07 - mae: 4.7305e-04\n",
      "Epoch 22: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6365 - loss: 3.5171e-07 - mae: 4.7305e-04 - val_acc: 0.6883 - val_loss: 5.3540e-07 - val_mae: 4.9710e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - acc: 0.6411 - loss: 3.5352e-07 - mae: 4.6543e-04\n",
      "Epoch 23: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.6409 - loss: 3.5358e-07 - mae: 4.6552e-04 - val_acc: 0.5896 - val_loss: 5.6997e-07 - val_mae: 5.8914e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6251 - loss: 3.4272e-07 - mae: 4.8041e-04\n",
      "Epoch 24: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - acc: 0.6253 - loss: 3.4310e-07 - mae: 4.8022e-04 - val_acc: 0.6273 - val_loss: 5.4875e-07 - val_mae: 5.5048e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6364 - loss: 3.4266e-07 - mae: 4.7133e-04\n",
      "Epoch 25: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6365 - loss: 3.4302e-07 - mae: 4.7132e-04 - val_acc: 0.6696 - val_loss: 5.4809e-07 - val_mae: 5.0633e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6392 - loss: 3.7038e-07 - mae: 4.7064e-04\n",
      "Epoch 26: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6393 - loss: 3.7001e-07 - mae: 4.7055e-04 - val_acc: 0.6823 - val_loss: 5.3141e-07 - val_mae: 5.0250e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - acc: 0.6409 - loss: 3.6218e-07 - mae: 4.6577e-04\n",
      "Epoch 27: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - acc: 0.6408 - loss: 3.6199e-07 - mae: 4.6580e-04 - val_acc: 0.6672 - val_loss: 5.3470e-07 - val_mae: 5.1429e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6458 - loss: 3.4514e-07 - mae: 4.6150e-04\n",
      "Epoch 28: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6458 - loss: 3.4534e-07 - mae: 4.6156e-04 - val_acc: 0.6876 - val_loss: 5.3334e-07 - val_mae: 4.9561e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6463 - loss: 3.9626e-07 - mae: 4.7306e-04\n",
      "Epoch 29: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6461 - loss: 3.9524e-07 - mae: 4.7293e-04 - val_acc: 0.6716 - val_loss: 5.4664e-07 - val_mae: 5.0547e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - acc: 0.6383 - loss: 3.2923e-07 - mae: 4.6872e-04\n",
      "Epoch 30: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.6383 - loss: 3.2985e-07 - mae: 4.6870e-04 - val_acc: 0.6845 - val_loss: 5.3735e-07 - val_mae: 4.9597e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - acc: 0.6365 - loss: 3.6368e-07 - mae: 4.7032e-04\n",
      "Epoch 31: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - acc: 0.6365 - loss: 3.6348e-07 - mae: 4.7032e-04 - val_acc: 0.6175 - val_loss: 5.8411e-07 - val_mae: 5.5438e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6170 - loss: 3.4245e-07 - mae: 4.9609e-04\n",
      "Epoch 32: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6174 - loss: 3.4293e-07 - mae: 4.9565e-04 - val_acc: 0.6884 - val_loss: 5.3353e-07 - val_mae: 4.9794e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6416 - loss: 3.5314e-07 - mae: 4.6665e-04\n",
      "Epoch 33: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6416 - loss: 3.5317e-07 - mae: 4.6665e-04 - val_acc: 0.6542 - val_loss: 5.3530e-07 - val_mae: 5.2252e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6448 - loss: 3.7789e-07 - mae: 4.6859e-04\n",
      "Epoch 34: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6448 - loss: 3.7725e-07 - mae: 4.6844e-04 - val_acc: 0.6213 - val_loss: 5.4701e-07 - val_mae: 5.5283e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6412 - loss: 3.8375e-07 - mae: 4.7144e-04\n",
      "Epoch 35: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6412 - loss: 3.8305e-07 - mae: 4.7141e-04 - val_acc: 0.5323 - val_loss: 6.3989e-07 - val_mae: 7.0390e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6178 - loss: 3.4980e-07 - mae: 4.9925e-04\n",
      "Epoch 36: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6181 - loss: 3.5000e-07 - mae: 4.9863e-04 - val_acc: 0.6483 - val_loss: 5.3575e-07 - val_mae: 5.2569e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - acc: 0.6389 - loss: 3.7479e-07 - mae: 4.7026e-04\n",
      "Epoch 37: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.6390 - loss: 3.7418e-07 - mae: 4.7006e-04 - val_acc: 0.6709 - val_loss: 5.2879e-07 - val_mae: 5.0451e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - acc: 0.6454 - loss: 3.3062e-07 - mae: 4.5597e-04\n",
      "Epoch 38: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.6454 - loss: 3.3109e-07 - mae: 4.5609e-04 - val_acc: 0.6777 - val_loss: 5.2981e-07 - val_mae: 5.0168e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - acc: 0.6484 - loss: 3.5498e-07 - mae: 4.5778e-04\n",
      "Epoch 39: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - acc: 0.6483 - loss: 3.5487e-07 - mae: 4.5792e-04 - val_acc: 0.6554 - val_loss: 5.3083e-07 - val_mae: 5.1637e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - acc: 0.6322 - loss: 3.3681e-07 - mae: 4.6779e-04\n",
      "Epoch 40: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - acc: 0.6323 - loss: 3.3716e-07 - mae: 4.6772e-04 - val_acc: 0.6583 - val_loss: 5.5825e-07 - val_mae: 5.2396e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - acc: 0.6321 - loss: 3.5650e-07 - mae: 4.7026e-04\n",
      "Epoch 41: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - acc: 0.6322 - loss: 3.5640e-07 - mae: 4.7017e-04 - val_acc: 0.5858 - val_loss: 5.6740e-07 - val_mae: 5.9615e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6288 - loss: 3.3067e-07 - mae: 4.7821e-04\n",
      "Epoch 42: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6291 - loss: 3.3125e-07 - mae: 4.7795e-04 - val_acc: 0.6491 - val_loss: 5.3336e-07 - val_mae: 5.2577e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - acc: 0.6431 - loss: 3.3928e-07 - mae: 4.6513e-04\n",
      "Epoch 43: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 92ms/step - acc: 0.6430 - loss: 3.3956e-07 - mae: 4.6511e-04 - val_acc: 0.6824 - val_loss: 5.2598e-07 - val_mae: 4.9973e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6382 - loss: 3.5068e-07 - mae: 4.6553e-04\n",
      "Epoch 44: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6384 - loss: 3.5066e-07 - mae: 4.6546e-04 - val_acc: 0.6682 - val_loss: 5.2876e-07 - val_mae: 5.0873e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6405 - loss: 3.8798e-07 - mae: 4.6605e-04\n",
      "Epoch 45: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6405 - loss: 3.8701e-07 - mae: 4.6596e-04 - val_acc: 0.6704 - val_loss: 5.4431e-07 - val_mae: 5.0211e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6300 - loss: 3.2460e-07 - mae: 4.7111e-04\n",
      "Epoch 46: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6301 - loss: 3.2532e-07 - mae: 4.7105e-04 - val_acc: 0.6890 - val_loss: 5.2789e-07 - val_mae: 4.9296e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6525 - loss: 3.4326e-07 - mae: 4.5647e-04\n",
      "Epoch 47: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - acc: 0.6524 - loss: 3.4341e-07 - mae: 4.5657e-04 - val_acc: 0.6908 - val_loss: 5.2885e-07 - val_mae: 4.9110e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6454 - loss: 3.6425e-07 - mae: 4.6712e-04\n",
      "Epoch 48: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6453 - loss: 3.6388e-07 - mae: 4.6700e-04 - val_acc: 0.6872 - val_loss: 5.2633e-07 - val_mae: 4.9561e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6528 - loss: 3.6500e-07 - mae: 4.6200e-04\n",
      "Epoch 49: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6525 - loss: 3.6458e-07 - mae: 4.6200e-04 - val_acc: 0.6847 - val_loss: 5.2381e-07 - val_mae: 4.9549e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6458 - loss: 3.4743e-07 - mae: 4.5968e-04\n",
      "Epoch 50: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6458 - loss: 3.4745e-07 - mae: 4.5969e-04 - val_acc: 0.6473 - val_loss: 5.3207e-07 - val_mae: 5.2211e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6463 - loss: 3.5253e-07 - mae: 4.5889e-04\n",
      "Epoch 51: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6463 - loss: 3.5240e-07 - mae: 4.5890e-04 - val_acc: 0.6853 - val_loss: 5.2555e-07 - val_mae: 4.9636e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - acc: 0.6357 - loss: 3.7288e-07 - mae: 4.7466e-04\n",
      "Epoch 52: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.6358 - loss: 3.7234e-07 - mae: 4.7448e-04 - val_acc: 0.6895 - val_loss: 5.2558e-07 - val_mae: 4.8742e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6418 - loss: 3.3091e-07 - mae: 4.6038e-04\n",
      "Epoch 53: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6418 - loss: 3.3135e-07 - mae: 4.6040e-04 - val_acc: 0.6643 - val_loss: 5.2627e-07 - val_mae: 5.0950e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6444 - loss: 3.7102e-07 - mae: 4.6168e-04\n",
      "Epoch 54: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6445 - loss: 3.7042e-07 - mae: 4.6163e-04 - val_acc: 0.6851 - val_loss: 5.2555e-07 - val_mae: 4.9322e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6546 - loss: 3.3091e-07 - mae: 4.5026e-04\n",
      "Epoch 55: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6545 - loss: 3.3128e-07 - mae: 4.5044e-04 - val_acc: 0.5798 - val_loss: 5.7183e-07 - val_mae: 6.0523e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6298 - loss: 3.6836e-07 - mae: 4.7556e-04\n",
      "Epoch 56: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6301 - loss: 3.6791e-07 - mae: 4.7532e-04 - val_acc: 0.6889 - val_loss: 5.3248e-07 - val_mae: 4.9106e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6453 - loss: 3.3446e-07 - mae: 4.6281e-04\n",
      "Epoch 57: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6452 - loss: 3.3481e-07 - mae: 4.6282e-04 - val_acc: 0.6921 - val_loss: 5.2561e-07 - val_mae: 4.8721e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6447 - loss: 3.2225e-07 - mae: 4.5737e-04\n",
      "Epoch 58: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6447 - loss: 3.2286e-07 - mae: 4.5744e-04 - val_acc: 0.6865 - val_loss: 5.2276e-07 - val_mae: 4.9137e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - acc: 0.6466 - loss: 3.8674e-07 - mae: 4.6455e-04\n",
      "Epoch 59: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - acc: 0.6466 - loss: 3.8571e-07 - mae: 4.6440e-04 - val_acc: 0.6935 - val_loss: 5.2928e-07 - val_mae: 4.8970e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6518 - loss: 3.1566e-07 - mae: 4.5366e-04\n",
      "Epoch 60: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6516 - loss: 3.1648e-07 - mae: 4.5386e-04 - val_acc: 0.6878 - val_loss: 5.2308e-07 - val_mae: 4.9088e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6455 - loss: 3.2633e-07 - mae: 4.5437e-04\n",
      "Epoch 61: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6455 - loss: 3.2680e-07 - mae: 4.5445e-04 - val_acc: 0.6852 - val_loss: 5.2049e-07 - val_mae: 4.9614e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - acc: 0.6416 - loss: 3.9038e-07 - mae: 4.6613e-04\n",
      "Epoch 62: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - acc: 0.6417 - loss: 3.8928e-07 - mae: 4.6596e-04 - val_acc: 0.6632 - val_loss: 5.2386e-07 - val_mae: 5.0659e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6456 - loss: 3.3071e-07 - mae: 4.5631e-04\n",
      "Epoch 63: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 82ms/step - acc: 0.6457 - loss: 3.3106e-07 - mae: 4.5637e-04 - val_acc: 0.6913 - val_loss: 5.2476e-07 - val_mae: 4.8862e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6385 - loss: 3.6793e-07 - mae: 4.6929e-04\n",
      "Epoch 64: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6386 - loss: 3.6744e-07 - mae: 4.6917e-04 - val_acc: 0.6792 - val_loss: 5.2198e-07 - val_mae: 4.9869e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6415 - loss: 3.8351e-07 - mae: 4.6903e-04\n",
      "Epoch 65: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6415 - loss: 3.8257e-07 - mae: 4.6879e-04 - val_acc: 0.6496 - val_loss: 5.2825e-07 - val_mae: 5.2549e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - acc: 0.6381 - loss: 3.6732e-07 - mae: 4.7310e-04\n",
      "Epoch 66: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.6382 - loss: 3.6685e-07 - mae: 4.7287e-04 - val_acc: 0.6908 - val_loss: 5.2266e-07 - val_mae: 4.8798e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - acc: 0.6445 - loss: 3.6849e-07 - mae: 4.6284e-04\n",
      "Epoch 67: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - acc: 0.6445 - loss: 3.6788e-07 - mae: 4.6269e-04 - val_acc: 0.6300 - val_loss: 5.4009e-07 - val_mae: 5.5165e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - acc: 0.6369 - loss: 3.8493e-07 - mae: 4.7367e-04\n",
      "Epoch 68: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - acc: 0.6369 - loss: 3.8404e-07 - mae: 4.7345e-04 - val_acc: 0.5844 - val_loss: 5.6338e-07 - val_mae: 5.8833e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - acc: 0.6242 - loss: 3.6698e-07 - mae: 4.8782e-04\n",
      "Epoch 69: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.6244 - loss: 3.6661e-07 - mae: 4.8739e-04 - val_acc: 0.6389 - val_loss: 5.3057e-07 - val_mae: 5.2604e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6465 - loss: 3.7463e-07 - mae: 4.6706e-04\n",
      "Epoch 70: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6464 - loss: 3.7387e-07 - mae: 4.6684e-04 - val_acc: 0.6912 - val_loss: 5.2243e-07 - val_mae: 4.8825e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6472 - loss: 3.4604e-07 - mae: 4.5680e-04\n",
      "Epoch 71: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6471 - loss: 3.4605e-07 - mae: 4.5695e-04 - val_acc: 0.6918 - val_loss: 5.1882e-07 - val_mae: 4.8905e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6446 - loss: 3.5445e-07 - mae: 4.5751e-04\n",
      "Epoch 72: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6445 - loss: 3.5423e-07 - mae: 4.5757e-04 - val_acc: 0.6791 - val_loss: 5.3993e-07 - val_mae: 4.9802e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6460 - loss: 3.6745e-07 - mae: 4.5568e-04\n",
      "Epoch 73: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6460 - loss: 3.6687e-07 - mae: 4.5569e-04 - val_acc: 0.6768 - val_loss: 5.1938e-07 - val_mae: 4.9622e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - acc: 0.6551 - loss: 3.2291e-07 - mae: 4.5489e-04\n",
      "Epoch 74: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.6550 - loss: 3.2342e-07 - mae: 4.5494e-04 - val_acc: 0.6597 - val_loss: 5.2365e-07 - val_mae: 5.0469e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6356 - loss: 3.5703e-07 - mae: 4.6248e-04\n",
      "Epoch 75: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6358 - loss: 3.5673e-07 - mae: 4.6241e-04 - val_acc: 0.6928 - val_loss: 5.2031e-07 - val_mae: 4.8617e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - acc: 0.6517 - loss: 3.1173e-07 - mae: 4.4989e-04\n",
      "Epoch 76: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.6516 - loss: 3.1254e-07 - mae: 4.5008e-04 - val_acc: 0.6428 - val_loss: 5.2717e-07 - val_mae: 5.2455e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6442 - loss: 3.3216e-07 - mae: 4.6027e-04\n",
      "Epoch 77: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6442 - loss: 3.3242e-07 - mae: 4.6023e-04 - val_acc: 0.6936 - val_loss: 5.2025e-07 - val_mae: 4.8443e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6481 - loss: 3.3210e-07 - mae: 4.5319e-04\n",
      "Epoch 78: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6480 - loss: 3.3238e-07 - mae: 4.5330e-04 - val_acc: 0.6720 - val_loss: 5.2139e-07 - val_mae: 4.9849e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - acc: 0.6434 - loss: 3.4055e-07 - mae: 4.5593e-04\n",
      "Epoch 79: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6435 - loss: 3.4063e-07 - mae: 4.5601e-04 - val_acc: 0.6700 - val_loss: 5.2019e-07 - val_mae: 5.0356e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6432 - loss: 3.5569e-07 - mae: 4.6038e-04\n",
      "Epoch 80: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6433 - loss: 3.5539e-07 - mae: 4.6034e-04 - val_acc: 0.6856 - val_loss: 5.2966e-07 - val_mae: 4.8894e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6454 - loss: 3.2890e-07 - mae: 4.5302e-04\n",
      "Epoch 81: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6454 - loss: 3.2927e-07 - mae: 4.5315e-04 - val_acc: 0.6759 - val_loss: 5.1985e-07 - val_mae: 4.9480e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - acc: 0.6505 - loss: 3.3344e-07 - mae: 4.5308e-04\n",
      "Epoch 82: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.6504 - loss: 3.3370e-07 - mae: 4.5323e-04 - val_acc: 0.6493 - val_loss: 5.3069e-07 - val_mae: 5.1306e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6476 - loss: 3.1376e-07 - mae: 4.5477e-04\n",
      "Epoch 83: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6476 - loss: 3.1449e-07 - mae: 4.5483e-04 - val_acc: 0.6723 - val_loss: 5.1768e-07 - val_mae: 5.0094e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6487 - loss: 3.7732e-07 - mae: 4.6488e-04\n",
      "Epoch 84: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6488 - loss: 3.7640e-07 - mae: 4.6464e-04 - val_acc: 0.6846 - val_loss: 5.2002e-07 - val_mae: 4.9132e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6450 - loss: 3.5007e-07 - mae: 4.6353e-04\n",
      "Epoch 85: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6450 - loss: 3.4991e-07 - mae: 4.6340e-04 - val_acc: 0.6948 - val_loss: 5.2068e-07 - val_mae: 4.8431e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6477 - loss: 3.2751e-07 - mae: 4.5249e-04\n",
      "Epoch 86: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6477 - loss: 3.2785e-07 - mae: 4.5253e-04 - val_acc: 0.6875 - val_loss: 5.3122e-07 - val_mae: 4.9084e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6482 - loss: 3.3163e-07 - mae: 4.5698e-04\n",
      "Epoch 87: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6482 - loss: 3.3186e-07 - mae: 4.5693e-04 - val_acc: 0.6970 - val_loss: 5.1919e-07 - val_mae: 4.8347e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6481 - loss: 3.4112e-07 - mae: 4.5671e-04\n",
      "Epoch 88: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6482 - loss: 3.4111e-07 - mae: 4.5662e-04 - val_acc: 0.6721 - val_loss: 5.1843e-07 - val_mae: 4.9528e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6406 - loss: 3.1931e-07 - mae: 4.5512e-04\n",
      "Epoch 89: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6408 - loss: 3.1983e-07 - mae: 4.5510e-04 - val_acc: 0.6752 - val_loss: 5.1971e-07 - val_mae: 4.9587e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6460 - loss: 2.9735e-07 - mae: 4.5193e-04\n",
      "Epoch 90: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6460 - loss: 2.9846e-07 - mae: 4.5205e-04 - val_acc: 0.6903 - val_loss: 5.1578e-07 - val_mae: 4.8340e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6476 - loss: 3.2706e-07 - mae: 4.5569e-04\n",
      "Epoch 91: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6477 - loss: 3.2739e-07 - mae: 4.5562e-04 - val_acc: 0.6708 - val_loss: 5.1621e-07 - val_mae: 5.0048e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - acc: 0.6492 - loss: 3.2838e-07 - mae: 4.5300e-04\n",
      "Epoch 92: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.6492 - loss: 3.2864e-07 - mae: 4.5301e-04 - val_acc: 0.6951 - val_loss: 5.2679e-07 - val_mae: 4.8690e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6528 - loss: 3.6769e-07 - mae: 4.6252e-04\n",
      "Epoch 93: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - acc: 0.6528 - loss: 3.6703e-07 - mae: 4.6232e-04 - val_acc: 0.6597 - val_loss: 5.1944e-07 - val_mae: 5.0215e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - acc: 0.6438 - loss: 3.7856e-07 - mae: 4.6105e-04\n",
      "Epoch 94: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6439 - loss: 3.7761e-07 - mae: 4.6090e-04 - val_acc: 0.6974 - val_loss: 5.2329e-07 - val_mae: 4.8380e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6464 - loss: 3.2321e-07 - mae: 4.5851e-04\n",
      "Epoch 95: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6463 - loss: 3.2365e-07 - mae: 4.5844e-04 - val_acc: 0.6629 - val_loss: 5.1955e-07 - val_mae: 5.0524e-04\n",
      "Epoch 96/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6526 - loss: 3.2484e-07 - mae: 4.5144e-04\n",
      "Epoch 96: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6524 - loss: 3.2528e-07 - mae: 4.5161e-04 - val_acc: 0.6668 - val_loss: 5.1909e-07 - val_mae: 4.9454e-04\n",
      "Epoch 97/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - acc: 0.6406 - loss: 3.8166e-07 - mae: 4.6423e-04\n",
      "Epoch 97: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6406 - loss: 3.8076e-07 - mae: 4.6417e-04 - val_acc: 0.6960 - val_loss: 5.2161e-07 - val_mae: 4.8361e-04\n",
      "Epoch 98/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6481 - loss: 3.2650e-07 - mae: 4.5318e-04\n",
      "Epoch 98: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6481 - loss: 3.2687e-07 - mae: 4.5325e-04 - val_acc: 0.6842 - val_loss: 5.2074e-07 - val_mae: 4.9201e-04\n",
      "Epoch 99/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - acc: 0.6454 - loss: 3.5898e-07 - mae: 4.6447e-04\n",
      "Epoch 99: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - acc: 0.6455 - loss: 3.5852e-07 - mae: 4.6426e-04 - val_acc: 0.6435 - val_loss: 5.2442e-07 - val_mae: 5.2359e-04\n",
      "Epoch 100/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6454 - loss: 3.2489e-07 - mae: 4.5204e-04\n",
      "Epoch 100: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - acc: 0.6453 - loss: 3.2528e-07 - mae: 4.5216e-04 - val_acc: 0.6896 - val_loss: 5.3081e-07 - val_mae: 4.9075e-04\n",
      "Epoch 101/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - acc: 0.6416 - loss: 3.4312e-07 - mae: 4.6362e-04\n",
      "Epoch 101: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - acc: 0.6416 - loss: 3.4308e-07 - mae: 4.6348e-04 - val_acc: 0.6709 - val_loss: 5.1761e-07 - val_mae: 4.9862e-04\n",
      "Epoch 102/200\n",
      "\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - acc: 0.6417 - loss: 3.4559e-07 - mae: 4.5767e-04\n",
      "Epoch 102: val_loss did not improve from 0.00000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.6418 - loss: 3.4547e-07 - mae: 4.5764e-04 - val_acc: 0.6450 - val_loss: 5.2454e-07 - val_mae: 5.2450e-04\n",
      "Epoch 103/200\n",
      "\u001b[1m20/79\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - acc: 0.6335 - loss: 3.2585e-07 - mae: 4.6972e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train,Y_train,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,validation_data\u001b[38;5;241m=\u001b[39m(X_test,Y_test),callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1558\u001b[0m   )\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,batch_size=512,epochs=200,validation_data=(X_test,Y_test),callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Mean Absolute Error (MAE): 0.00039074026504879453\n",
      "Mean Squared Error (MSE): 2.1097288199168353e-07\n",
      "R-squared (R²): 0.5879716360432098\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('AIpred0.01.keras', custom_objects={\n",
    "    'acc': acc,\n",
    "    'custom_activation': CustomActivation\n",
    "})\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# คำนวณ metrics\n",
    "mae = mean_absolute_error(Y_test[-30:], model.predict(X_test[-30:]))\n",
    "mse = mean_squared_error(Y_test[-30:], model.predict(X_test[-30:]))\n",
    "r2 = r2_score(Y_test[-30:], model.predict(X_test[-30:]))\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R²):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "Mean Absolute Error (MAE): 0.0004771693309646795\n",
      "Mean Squared Error (MSE): 9.945369756287739e-07\n",
      "R-squared (R²): 0.2572676782420825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# คำนวณ metrics\n",
    "mae = mean_absolute_error(Y_test, model.predict(X_test))\n",
    "mse = mean_squared_error(Y_test, model.predict(X_test))\n",
    "r2 = r2_score(Y_test, model.predict(X_test))\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R²):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0011495905850038568, array([0.00036729], dtype=float32))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Y_test)[4],model.predict(X_test)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.81      0.87        16\n",
      "        True       0.81      0.93      0.87        14\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.87      0.87      0.87        30\n",
      "weighted avg       0.87      0.87      0.87        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test[-30:]>0,model.predict(X_test[-30:])>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl1=data[[\"diff_n_o\",\"diff_n_l\",\"diff_n_h\",\"diff_n_c\"]].iloc[1:,:]\n",
    "Xl2=data[[\"diff_n_o\",\"diff_n_l\",\"diff_n_h\",\"diff_n_c\"]].iloc[0:-1,:]\n",
    "x=np.concatenate([Xl1,Xl2],axis=-1)\n",
    "timestep=10\n",
    "X=[]\n",
    "for i in range(len(x)-timestep+1):\n",
    "    c=np.array(x[i:i+timestep])\n",
    "    X.append(c)\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00059246]], dtype=float32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[-1][np.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.59      0.69      3010\n",
      "        True       0.62      0.83      0.71      2437\n",
      "\n",
      "    accuracy                           0.70      5447\n",
      "   macro avg       0.72      0.71      0.70      5447\n",
      "weighted avg       0.73      0.70      0.70      5447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test>0,model.predict(X_test)>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_min = tf.minimum(inputs[:, :tf.shape(inputs)[1] // 2], 0)\n",
    "        x_max = tf.maximum(inputs[:, tf.shape(inputs)[1] // 2:], 0)\n",
    "        return tf.concat([x_min, x_max], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_113\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_113\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">267,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_271 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_272 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_273 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_357 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2349</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_124 (\u001b[38;5;33mSimpleRNN\u001b[0m)      │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m267,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_271 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_125 (\u001b[38;5;33mSimpleRNN\u001b[0m)      │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m196,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_272 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_126 (\u001b[38;5;33mSimpleRNN\u001b[0m)      │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_273 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_127 (\u001b[38;5;33mSimpleRNN\u001b[0m)      │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m127\u001b[0m)            │        \u001b[38;5;34m48,768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_357 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;34m2349\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,934,594</span> (7.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,934,594\u001b[0m (7.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">644,864</span> (2.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m644,864\u001b[0m (2.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,289,730</span> (4.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,289,730\u001b[0m (4.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 8 and 10 for '{{node sequential_113_1/simple_rnn_124_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_113_1/simple_rnn_124_1/strided_slice_1, sequential_113_1/simple_rnn_124_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [32,8], [10,512].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(32, 8), dtype=float32)\n  • states=('tf.Tensor(shape=(32, 512), dtype=float32)',)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m m \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredictHmodel3.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, custom_objects\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m: acc,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_activation\u001b[39m\u001b[38;5;124m'\u001b[39m: CustomActivation\n\u001b[1;32m      5\u001b[0m })\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#predict\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m pred\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 8 and 10 for '{{node sequential_113_1/simple_rnn_124_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_113_1/simple_rnn_124_1/strided_slice_1, sequential_113_1/simple_rnn_124_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [32,8], [10,512].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(32, 8), dtype=float32)\n  • states=('tf.Tensor(shape=(32, 512), dtype=float32)',)\n  • training=False"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "m = tf.keras.models.load_model('PredictHmodel3.keras', custom_objects={\n",
    "    'acc': acc,\n",
    "    'custom_activation': CustomActivation\n",
    "})\n",
    "\n",
    "\n",
    "#predict\n",
    "pred=m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.48      0.56        50\n",
      "        True       0.68      0.82      0.74        67\n",
      "\n",
      "    accuracy                           0.68       117\n",
      "   macro avg       0.67      0.65      0.65       117\n",
      "weighted avg       0.67      0.68      0.66       117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train>0,pred>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12     0.019211\n",
       "13     0.041705\n",
       "14     0.021371\n",
       "15     0.010146\n",
       "16     0.001749\n",
       "         ...   \n",
       "124    0.012674\n",
       "125    0.021022\n",
       "126    0.024392\n",
       "127    0.017202\n",
       "128    0.006699\n",
       "Name: diff_n_OH, Length: 117, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "0.675 80\n"
     ]
    }
   ],
   "source": [
    "acc = 0  # นับจำนวนครั้งที่ทำนายถูกต้อง\n",
    "tot = 0  # นับจำนวนตัวอย่างทั้งหมดที่เข้าเงื่อนไข\n",
    "Y1=data[\"diff_n_c\"].iloc[timestep:]\n",
    "Y_train1=Y1[1:2350]\n",
    "Y_test1=Y1[2350:]\n",
    "\n",
    "k=data[\"diff_n_h\"].iloc[timestep:]\n",
    "kk=k[2350:]\n",
    "\n",
    "k2=data[\"diff_n_l\"].iloc[timestep:]\n",
    "kk2=k2[2350:]\n",
    "# การทำนายผลลัพธ์ของ model และ model2\n",
    "pred1 = m.predict(X_train)##ทำนายค่าสูงสุด\n",
    "\n",
    "# ลูปตรวจสอบผลลัพธ์\n",
    "for i in range(1,len(Y_train)):\n",
    "    # เช็คว่า pred1 และ pred2 ทั้งคู่ทำนายค่ามากกว่า 0\n",
    "    if (pred1[i] > 0) :\n",
    "        if (np.array(Y_train)[i] > 0):  # ถ้าค่าจริงของ Y_test > 0\n",
    "            acc += 1\n",
    "            tot += 1  # เพิ่ม tot ไม่ว่าจะทำนายถูกหรือไม่\n",
    "        else:  # เช็คค่าที่เหลือสำหรับ Y_test < 0\n",
    "            tot += 1\n",
    "print(acc/tot, tot) ### ข้อสรุปคือราคาสูงสุดที่โมเดลทำนายกับราคาสูงสุดของเมื่อวานเเละก่อนหน้าทำนายราคาปิดได้\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
